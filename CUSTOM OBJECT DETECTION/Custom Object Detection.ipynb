{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B952917790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B953EC4C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    _, img = cap.read()\n",
    "    \n",
    "    model = load_model(r'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\mobile_pen.h5')\n",
    "    #img = cv2.imread(r\"C:\\Users\\prash\\Desktop\\data\\cotton plant leaf disease\\train2\\diseased cotton leaf\\dis_leaf (31)_iaip.jpg\")\n",
    "    img1 = img.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    pred = np.argmax(model.predict(img))\n",
    "    print(pred)\n",
    "    \n",
    "    \n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img1 = cv2.resize(img1, (224,224))\n",
    "    lower = np.array([60,60,60])\n",
    "    higher = np.array([250,250,250])\n",
    "    mask = cv2.inRange(img1, lower, higher)\n",
    "    cnt,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    cnt_img = cv2.drawContours(img1, cnt, -1,255,3)\n",
    "    c = max(cnt, key = cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    \n",
    "    if (pred == [0]):\n",
    "        name = \"Diseased cotton leaf\"\n",
    "        cv2.rectangle(img1, (x,y),(x+w, y+h),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (x,y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    elif (pred == [1]):\n",
    "        name = \"Diseased cotton plant\"\n",
    "        cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (17,17), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    elif (pred == [2]):\n",
    "        name = \"Fresh cotton leaf\"\n",
    "        cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (17,17), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    else:\n",
    "        name = \"Fresh cotton plant\"\n",
    "        cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (17,17), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Video\", img1)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    _, img = cap.read()\n",
    "    \n",
    "    model = load_model(r'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\mobile_pen.h5')\n",
    "    img = cv2.imread(r\"F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\Pen\\pen 1.jpg\")\n",
    "    img1 = img.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    pred = np.argmax(model.predict(img))\n",
    "    print(pred)\n",
    "    \n",
    "    \n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img1 = cv2.resize(img1, (224,224))\n",
    "    #lower = np.array([120,120,120])\n",
    "    #higher = np.array([250,250,250])\n",
    "    #mask = cv2.inRange(img1, lower, higher)\n",
    "    #cnt,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    #cnt_img = cv2.drawContours(img1, cnt, -1,255,3)\n",
    "    #c = max(cnt, key = cv2.contourArea)\n",
    "    #x,y,w,h = cv2.boundingRect(c)\n",
    "    \n",
    "    if (pred == [0]):\n",
    "        name = \"Mobile\"\n",
    "        cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (17,17), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    else:\n",
    "        name = \"Pen\"\n",
    "        cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (17,17), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"Video\", img1)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000223776301F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022377630DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, img = cap.read()\n",
    "\n",
    "    model = load_model(r'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\mobile_pen.h5')\n",
    "    #img = cv2.imread(r\"F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\Pen\\pen 1.jpg\")\n",
    "    img1 = img.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    pred = np.argmax(model.predict(img))\n",
    "    #print(pred)\n",
    "\n",
    "\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img1 = cv2.resize(img1, (224,224))\n",
    "    '''lower = np.array([120,120,120])\n",
    "    higher = np.array([250,250,250])\n",
    "    mask = cv2.inRange(img1, lower, higher)\n",
    "    cnt,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    cnt_img = cv2.drawContours(img1, cnt, -1,255,3)\n",
    "    c = max(cnt, key = cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(c)'''\n",
    "\n",
    "    if (pred == [0]):\n",
    "        name = \"Mobile\"\n",
    "        cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (20,20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    else:\n",
    "        name = \"Pen\"\n",
    "        cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img1, name, (20,20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Video\", img1)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2390cd826800>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\mobile_pen.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\Mobile\\mobile 10.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "#while True:\n",
    "\n",
    "#_, img = cap.read()\n",
    "\n",
    "model = load_model(r'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\mobile_pen.h5')\n",
    "img = cv2.imread(r\"F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\Mobile\\mobile 10.jpg\")\n",
    "img1 = img.copy()\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (224,224))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = preprocess_input(img)\n",
    "pred = np.argmax(model.predict(img))\n",
    "#print(pred)\n",
    "\n",
    "\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img1 = cv2.resize(img1, (224,224))\n",
    "'''lower = np.array([120,120,120])\n",
    "higher = np.array([250,250,250])\n",
    "mask = cv2.inRange(img1, lower, higher)\n",
    "cnt,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "cnt_img = cv2.drawContours(img1, cnt, -1,255,3)\n",
    "c = max(cnt, key = cv2.contourArea)\n",
    "x,y,w,h = cv2.boundingRect(c)'''\n",
    "\n",
    "if (pred == [0]):\n",
    "    name = \"Mobile\"\n",
    "    cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "    cv2.putText(img1, name, (x,y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "else:\n",
    "    name = \"Pen\"\n",
    "    cv2.rectangle(img1, (20,20),(210, 210),(255,0,0),2)\n",
    "    cv2.putText(img1, name, (20,20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "\n",
    "cv2.imshow(\"Video\", img1)\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-59730169af9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0485258\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Mobile\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m210\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m210\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "classifier = cv2.CascadeClassifier(r'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\classifier/cascade.xml')\n",
    "\n",
    "while True:\n",
    "\n",
    "    _, img = cap.read()\n",
    "\n",
    "    img = cv2.resize(img,(32,32))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = classifier.detectMultiScale(img, 1.0485258, 6)\n",
    "\n",
    "    if (pred == [0]):\n",
    "        name = \"Mobile\"\n",
    "        cv2.rectangle(img, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img, name, (x,y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "    else:\n",
    "        name = \"Pen\"\n",
    "        cv2.rectangle(img, (20,20),(210, 210),(255,0,0),2)\n",
    "        cv2.putText(img, name, (20,20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "    cv2.imshow(\"Video\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for (x,y,w,h) in img:\n",
    "        if (pred == [0]):\n",
    "            name = \"Mobile\"\n",
    "            cv2.rectangle(img, (x,y),(x+w, y+h),(255,0,0),2)\n",
    "            cv2.putText(img, name, (x,y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n",
    "        else:\n",
    "            name = \"Pen\"\n",
    "            cv2.rectangle(img, (x,y),(x+w, y+h),(255,0,0),2)\n",
    "            cv2.putText(img, name, (20,20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " ...\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "classifier = cv2.CascadeClassifier(r'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\classifier/cascade.xml')\n",
    "img = cv2.imread(r\"F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\p\\1639935019368.jpg\",cv2.IMREAD_UNCHANGED)\n",
    "#scale_percent = 40\n",
    "#width = int(img.shape[1] * scale_percent/100)\n",
    "#height = int(img.shape[0] * scale_percent/100)\n",
    "#dim = (width, height)\n",
    "#img = cv2.resize(img,dim,interpolation=cv2.INTER_AREA)\n",
    "img = cv2.resize(img,(32,32))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print(img)\n",
    "img = classifier.detectMultiScale(img, 1.5, 4)\n",
    "print(img)\n",
    "#if img is ():\n",
    " #   print(\"No objects found\")\n",
    "\n",
    "for (x,y,w,h) in img:\n",
    "    cv2.rectangle(img, (x,y), (x+w,y+h), (127,0,255), 2)\n",
    "    cv2.imshow('object Detection', img)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "classifier = cv2.CascadeClassifier(r'F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\classifier/cascade.xml')\n",
    "img = cv2.imread(r\"F:\\MACHINE_LEARNING\\CUSTOM_OBJECT_DETECTION\\train\\p\\1639935019368.jpg\",cv2.IMREAD_UNCHANGED)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
